\subsubsection{Temporal Convolutional Networks (TCN) and Variants with Sparse Attention (TCAN)}

The TCN model was implemented to predict the log-transformed annual maximum rainfall intensity using sequences of log-transformed return period and duration as input features. Each input was formatted as a sequence of length 3, with features standardized for stable training. The model architecture consists of two dilated 1D convolutional layers with residual connections, followed by global average pooling and a linear output layer, resulting in a highly parameter-efficient design.

\vspace{1em}

Training was performed using the AdamW optimizer with a learning rate of 0.001 and weight decay of 1e-5, for up to 1000 epochs with early stopping based on validation loss. Weighted mean squared error was used as the loss function to emphasize target IDF points, and gradient clipping was applied for stability. Model performance was assessed using RMSE, MAE, and R² metrics, and the generated IDF curves were compared against the Gumbel-distribution-based benchmarks for validation.

\vspace{1em}

The TCN leverages dilated convolutions and residual connections to efficiently capture both local and long-range dependencies in the duration-intensity relationship, which is essential for modeling the complex temporal structure of rainfall events. Its lightweight architecture enables high accuracy with minimal parameters, making it suitable for hydrological modeling tasks with limited data.

\vspace{1em}

The TCAN model extends the TCN\@ by incorporating a lightweight multi-head self-attention mechanism after the dilated convolutional layers. This addition allows the network to better capture long-range dependencies and interactions between different temporal positions in the input sequence. The input preparation and standardization procedures mirror those used for the TCN\@.

\vspace{1em}

Training of the TCAN followed the same protocol as the TCN, utilizing the AdamW optimizer, early stopping, and weighted loss to ensure robust generalization and prevent overfitting. Model evaluation was conducted using the same suite of metrics (RMSE, MAE, R²), and the predicted IDF curves were benchmarked against the Gumbel-based results.

\vspace{1em}

The integration of sparse attention enables the TCAN to focus on the most relevant features across the input sequence, enhancing its ability to model complex, nonlinear relationships in rainfall data. This makes the TCAN particularly effective for generating accurate IDF curves, especially in scenarios where capturing subtle temporal dependencies is critical.
